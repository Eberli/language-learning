#ASuMa, Mar 2018

Directory for tokenizer tools

Tokenization is done by tokenizer.py, using LG 'any' language dictionary. 
tokenizer.py takes a file previously processed by sentences splitting, and optionally pre-cleaning. 
Main function documents arguments; they're copied here:

Tokenizer procedure that uses LG tokenizer with python bindings

        Usage: tokenizer.py -i <inputfile> -o <outputfile>

        inputfile           Name of inputfile
        outputfile          Name of ouputfile
