{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Basic-settings\" data-toc-modified-id=\"Basic-settings-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Basic settings</a></span></li><li><span><a href=\"#Child-Directed-Speech-~-fast-check\" data-toc-modified-id=\"Child-Directed-Speech-~-fast-check-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Child Directed Speech ~ fast check</a></span><ul class=\"toc-item\"><li><span><a href=\"#VSM\" data-toc-modified-id=\"VSM-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>VSM</a></span></li><li><span><a href=\"#Clustering-*2\" data-toc-modified-id=\"Clustering-*2-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Clustering *2</a></span></li><li><span><a href=\"#Grammar\" data-toc-modified-id=\"Grammar-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Grammar</a></span></li></ul></li><li><span><a href=\"#Gutenberg-Children-Books-Caps\" data-toc-modified-id=\"Gutenberg-Children-Books-Caps-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Gutenberg-Children-Books-Caps</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative clustering in sparse vector space\n",
    "Code implemented in development branch, waiting for merge after baseline tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T15:28:22.051854Z",
     "start_time": "2018-10-12T15:28:21.384838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-12 15:28:22 UTC \n",
      "/home/obaskov/py/language-learning/output/Agglomerative_Clustering_2018-10-12\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import linalg\n",
    "from IPython.display import display\n",
    "from ull.grammar_learner.utl import UTC\n",
    "from ull.grammar_learner.read_files import check_dir, check_mst_files\n",
    "from ull.grammar_learner.pparser import files2links\n",
    "from ull.grammar_learner.clustering import cluster_id \n",
    "from ull.grammar_learner.widgets import html_table\n",
    "from ull.grammar_learner.write_files import list2file, save_link_grammar, save_cat_tree\n",
    "from ull.grammar_learner.pqa_table import params, pqa_meter\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path: sys.path.append(module_path)\n",
    "# Updated modules:\n",
    "from src.grammar_learner.category_learner import \\\n",
    "    learn_categories, add_disjuncts, cats2list, cdf2cats\n",
    "from src.grammar_learner.grammar_inducer import induce_grammar\n",
    "# New (development) modules:\n",
    "from src.grammar_learner_.sparse_word_space_ import \\\n",
    "    clean_links, co_occurrence_matrix, categorical_distribution\n",
    "from src.grammar_learner_.agglomerative_clustering_ import agglomerative_clustering\n",
    "start = time.time()\n",
    "out_dir = module_path + '/output/Agglomerative_Clustering_' + str(UTC())[:10]\n",
    "check_dir(out_dir, True, 'none')\n",
    "tmpath = out_dir + '/tmp/'\n",
    "check_dir(tmpath, True, 'none')\n",
    "print(UTC(), '\\n' + out_dir)\n",
    "# cluster_id(26,26)  # Check (new) cluster numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T15:28:22.060540Z",
     "start_time": "2018-10-12T15:28:22.053782Z"
    }
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'min_word_count':   1           ,   # SVS (sparse vector space) parameter\n",
    "    'min_link_count':   1           ,   # SVS\n",
    "    'max_words'     :   100000      ,   # SVS: max space dimension 1\n",
    "    'max_features'  :   100000      ,   # SVS: dimension 2: disjuncts/connectors \n",
    "    'min_co-occurrence_count':  3   ,   # SVS\n",
    "    'min_co-occurrence_probability': 1e-9,  # SVS\n",
    "    'left_wall'     :   ''          ,\n",
    "    'period'        :   False       ,\n",
    "    'context'       :   1           ,\n",
    "    'word_space'    :   'sparse'    ,\n",
    "    'clustering'    :   ('agglomerative', 'ward'),      # new \n",
    "    'cluster_range' :   (30,120,3,3),   # min, max, step, repeat \n",
    "    'cluster_criteria': 'silhouette',   # legacy, to be replaced with:\n",
    "    'clustering_metric': ('silhouette', 'euclidean'),   # new option (?)\n",
    "    'cluster_level' :   1           ,\n",
    "    'grammar_rules' :   2           ,\n",
    "    'tmpath'        :   tmpath      , \n",
    "    'verbose'       :   'min'       ,\n",
    "    'template_path' :   'poc-turtle',\n",
    "    'linkage_limit' :   1000        ,\n",
    "    'categories_generalization': 'off' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Child Directed Speech ~ fast check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T15:28:23.903017Z",
     "start_time": "2018-10-12T15:28:22.062470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_links: min_word_count: 3, min_link_count: 1\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "corpus = 'CDS-caps-br-text+brent9mos'\n",
    "#corpus = 'Gutenberg-Children-Books-Caps'\n",
    "dataset = 'LG-English'\n",
    "kwargs['min_word_count'] = 3   # 10 default 81010\n",
    "kwargs['min_link_count'] = 1    # 1 default 81010\n",
    "kwargs['min_co-occurrence_count'] = 1\n",
    "#kwargs['min_co-occurrence_probability'] = 1e-3  # 0 default ~ categorical distribution threshold\n",
    "ip, oc, og = params(corpus, dataset, module_path, out_dir, **kwargs)\n",
    "rp = module_path + '/data/CDS-caps-br-text+brent9mos/LG-English'\n",
    "#rp = ip  # reference path = Gutenberg-Children-Books-Caps/LG-English\n",
    "cp = rp  # corpus path = reference_path :: use 'gold' parses as test corpus\n",
    "files, re01 = check_mst_files(ip, verbose='none')\n",
    "kwargs['input_files'] = files\n",
    "links, re02 = files2links(**kwargs)\n",
    "linx, words, features = clean_links(links, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T15:28:24.042742Z",
     "start_time": "2018-10-12T15:28:23.904880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49570 links: 3306 unique words, 5285 links\n",
      "words: len 2222, min 'd, max zoo\n",
      "features: len 3981, min 'd+, max zoo-\n",
      "linx[0]: len 2222, min 0, max 2221\n",
      "linx[1]: len 3965, min 0, max 3980\n",
      "co_occurrence_matrix: threshold = 1\n",
      "categorical_distribution: threshold = 0.001\n",
      "counts.shape (2222, 3981),cd.shape (2222, 3981)\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(links)} links: {len(set(links[\"word\"].tolist()))} unique words, {len(set(links[\"link\"].tolist()))} links')\n",
    "print(f'words: len {len(words)}, min {min(words)}, max {max(words)}')\n",
    "print(f'features: len {len(features)}, min {min(features)}, max {max(features)}')\n",
    "l0 = np.unique(linx[:,0])\n",
    "l1 = np.unique(linx[:,1])\n",
    "print(f'linx[0]: len {len(l0)}, min {min(l0)}, max {max(l0)}')\n",
    "print(f'linx[1]: len {len(l1)}, min {min(l1)}, max {max(l1)}')\n",
    "counts = co_occurrence_matrix(linx, **kwargs)\n",
    "cd = categorical_distribution(counts, **kwargs)\n",
    "print(f'counts.shape {counts.shape},cd.shape {cd.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering *2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T15:28:24.047600Z",
     "start_time": "2018-10-12T15:28:24.044190Z"
    }
   },
   "outputs": [],
   "source": [
    "def cluster_sizes(cdf):\n",
    "    cs = {}\n",
    "    for x in cdf['cluster_words'].tolist():\n",
    "        if len(x) in cs: cs[len(x)] += 1\n",
    "        else: cs[len(x)] = 1\n",
    "    return sorted(cs, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T15:28:57.335761Z",
     "start_time": "2018-10-12T15:28:24.051125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agglomerative: 5 clusters [2204, 8, 1], SI 0.983, VR 658\n",
      "agglomerative: 6 clusters [2201, 8, 3, 1], SI 0.985, VR 704\n",
      "agglomerative: 7 clusters [2201, 8, 7, 3, 1], SI 0.985, VR 704\n",
      "agglomerative: 8 clusters [2201, 8, 5, 3, 2, 1], SI 0.986, VR 693\n",
      "agglomerative: 9 clusters [2200, 8, 5, 3, 2, 1], SI 0.986, VR 700\n"
     ]
    }
   ],
   "source": [
    "kwargs['clustering'] = ('agglomerative', 'ward')\n",
    "for n_clusters in range(5,10):\n",
    "    cdf, silhouette, m2, labels = agglomerative_clustering(cd, words, n_clusters, **kwargs)\n",
    "    print(f'{kwargs[\"clustering\"][0]}: {n_clusters} clusters {cluster_sizes(cdf)}, SI {round(silhouette, 3)}, VR {int(m2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T15:28:57.352512Z",
     "start_time": "2018-10-12T15:28:57.337354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>[the, to, a, and, get, in, put, i, see, we, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>[that, it, there, he, where]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>[is, do, are]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E</td>\n",
       "      <td>[on, here]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F</td>\n",
       "      <td>['s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>G</td>\n",
       "      <td>[go, can, 're, want, gonna, don't, wanna, can't]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H</td>\n",
       "      <td>[what]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I</td>\n",
       "      <td>[you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>J</td>\n",
       "      <td>[no]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cluster                                      cluster_words\n",
       "0       B  [the, to, a, and, get, in, put, i, see, we, ha...\n",
       "1       C                       [that, it, there, he, where]\n",
       "2       D                                      [is, do, are]\n",
       "3       E                                         [on, here]\n",
       "4       F                                               ['s]\n",
       "5       G   [go, can, 're, want, gonna, don't, wanna, can't]\n",
       "6       H                                             [what]\n",
       "7       I                                              [you]\n",
       "8       J                                               [no]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T15:29:56.188115Z",
     "start_time": "2018-10-12T15:28:57.354028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agglomerative: 5 clusters [2204, 8, 1], SI 0.983, VR 658\n",
      "agglomerative: 10 clusters [2199, 8, 5, 3, 2, 1], SI 0.987, VR 726\n",
      "agglomerative: 20 clusters [2192, 8, 3, 2, 1], SI 0.99, VR 1736\n",
      "agglomerative: 21 clusters [2191, 8, 3, 2, 1], SI 0.99, VR 2211\n",
      "agglomerative: 22 clusters [2191, 7, 3, 2, 1], SI 0.99, VR 3013\n",
      "agglomerative: 23 clusters [2191, 6, 3, 2, 1], SI 0.99, VR 4841\n",
      "agglomerative: 24 clusters [2191, 5, 3, 2, 1], SI 0.991, VR 12502\n",
      "agglomerative: 25 clusters [2191, 5, 3, 2, 1], SI 0.991, VR 1\n",
      "agglomerative: 30 clusters [2191, 2, 1], SI nan, VR 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obaskov/miniconda3/envs/ull_dev/lib/python3.6/site-packages/sklearn/metrics/cluster/unsupervised.py:205: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sil_samples /= np.maximum(intra_clust_dists, inter_clust_dists)\n"
     ]
    }
   ],
   "source": [
    "for n_clusters in [5,10,20,21,22,23,24,25,30]:\n",
    "    cdf, silhouette, m2, labels = agglomerative_clustering(cd, words, n_clusters, **kwargs)\n",
    "    print(f'{kwargs[\"clustering\"][0]}: {n_clusters} clusters {cluster_sizes(cdf)}, SI {round(silhouette, 3)}, VR {int(m2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:00:53.875978Z",
     "start_time": "2018-10-11T14:00:50.127Z"
    }
   },
   "source": [
    "### Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T15:30:02.860784Z",
     "start_time": "2018-10-12T15:29:56.189722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agglomerative: 7 clusters [2201, 8, 7, 3, 1], SI 0.985, VR = 704\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 7\n",
    "cdf, silhouette, m2, labels = agglomerative_clustering(cd, words, n_clusters, **kwargs)\n",
    "print(f'{kwargs[\"clustering\"][0]}: {n_clusters} clusters {cluster_sizes(cdf)},\\\n",
    " SI {round(silhouette, 3)}, VR = {int(m2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T15:30:04.634918Z",
     "start_time": "2018-10-12T15:30:02.862458Z"
    }
   },
   "outputs": [],
   "source": [
    "if kwargs['context'] == 1 and kwargs['grammar_rules'] == 2:\n",
    "    context = kwargs['context']\n",
    "    kwargs['context'] = kwargs['grammar_rules']\n",
    "    links, re06 = files2links(**kwargs)\n",
    "    kwargs['context'] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T15:30:05.061070Z",
     "start_time": "2018-10-12T15:30:04.637022Z"
    }
   },
   "outputs": [],
   "source": [
    "cats = cdf2cats(cdf)\n",
    "gen_cats = cats\n",
    "fat_cats = add_disjuncts(gen_cats, links, verbose='none')\n",
    "rules, re07 = induce_grammar(fat_cats, links)\n",
    "re09 = save_cat_tree(rules,  oc, verbose='none')\n",
    "grammar_rules = 2\n",
    "re10 = save_link_grammar(rules, og, grammar_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T15:35:39.693878Z",
     "start_time": "2018-10-12T15:30:05.062362Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "pa, pq, qa = pqa_meter(re10['grammar_file'], og, cp, rp, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T15:35:39.701884Z",
     "start_time": "2018-10-12T15:35:39.695281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA/PQ = 70.9%/49.8%\n"
     ]
    }
   ],
   "source": [
    "print(f'PA/PQ = {round(pa,1)}%/{round(pq,1)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T15:35:39.713786Z",
     "start_time": "2018-10-12T15:35:39.704408Z"
    }
   },
   "outputs": [],
   "source": [
    "#STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gutenberg-Children-Books-Caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-12T15:36:58.282Z"
    }
   },
   "outputs": [],
   "source": [
    "def gogo(n_clusters, ip, oc, og, cp, rp, **kwargs):\n",
    "    files, re01 = check_mst_files(ip, verbose='none')\n",
    "    kwargs['input_files'] = files\n",
    "    links, re02 = files2links(**kwargs)\n",
    "    linx, words, features = clean_links(links, **kwargs)\n",
    "    counts = co_occurrence_matrix(linx, **kwargs)\n",
    "    cd = categorical_distribution(counts, **kwargs)\n",
    "    print(f'counts.shape: {counts.shape}, cd.shape: {cd.shape}')\n",
    "    print(f'{len(links)} links: {len(set(links[\"word\"].tolist()))} unique words, {len(set(links[\"link\"].tolist()))} links')\n",
    "    print(f'words: len {len(words)}, min {min(words)}, max {max(words)}')\n",
    "    print(f'features: len {len(features)}, min {min(features)}, max {max(features)}')\n",
    "    l0 = np.unique(linx[:,0])\n",
    "    l1 = np.unique(linx[:,1])\n",
    "    print(f'linx[0]: len {len(l0)}, min {min(l0)}, max {max(l0)}')\n",
    "    print(f'linx[1]: len {len(l1)}, min {min(l1)}, max {max(l1)}')\n",
    "    cdf, silhouette, m2, labels = agglomerative_clustering(cd, words, n_clusters, **kwargs)\n",
    "    print(f'{kwargs[\"clustering\"][0]}: {n_clusters} clusters, SI {round(silhouette, 3)}, VR = {int(m2)}')\n",
    "    print('cluster sizes:', cluster_sizes(cdf))\n",
    "    cats = cdf2cats(cdf)\n",
    "    gen_cats = cats\n",
    "    if kwargs['context'] == 1 and kwargs['grammar_rules'] == 2:\n",
    "        context = kwargs['context']\n",
    "        kwargs['context'] = kwargs['grammar_rules']\n",
    "        links, re06 = files2links(**kwargs)\n",
    "        kwargs['context'] = context\n",
    "    fat_cats = add_disjuncts(gen_cats, links, verbose='none')\n",
    "    rules, re07 = induce_grammar(fat_cats, links)\n",
    "    re09 = save_cat_tree(rules,  oc, verbose='none')\n",
    "    grammar_rules = 2\n",
    "    re10 = save_link_grammar(rules, og, kwargs['grammar_rules'])\n",
    "    pa, pq, qa = pqa_meter(re10['grammar_file'], og, cp, rp, **kwargs)\n",
    "    return pa, pq, cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-12T15:37:12.690Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = 'Gutenberg-Children-Books-Caps'\n",
    "dataset = 'LG-English'\n",
    "ip, oc, og = params(corpus, dataset, module_path, out_dir, **kwargs)\n",
    "rp = ip  # reference path = LG-English\n",
    "cp = rp  # corpus path = reference path\n",
    "kwargs['min_word_count'] = 31\n",
    "kwargs['min_link_count'] = 2\n",
    "kwargs['min_co-occurrence_count'] = 1\n",
    "kwargs['min_co-occurrence_probability'] = 1e-9  # categorical distribution threshold\n",
    "kwargs['clustering'] = ('agglomerative', 'ward') # linkage: ('ward', 'average', 'complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-12T15:37:17.562Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "n_clusters = 100\n",
    "pa, pq, cdf = gogo(n_clusters, ip, oc, og, cp, rp, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-12T15:37:19.570Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'PA/PQ = {round(pa,1)}%/{round(pq,1)}%')\n",
    "print('cluster sizes:', cluster_sizes(cdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st take: **1-connector-disjunct rules**:  \n",
    "PA/PQ = 14.1%/5.5%  \n",
    "cluster sizes: [1703, 1085, 875, 732, 703, 276, 269, 202, 172, 156, 111, 109, 83, 72, 51, 37, 26, 21, 19, 17, 11, 10, 8, 7, 5, 4, 3, 2, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229.865px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
