{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC-Turtle-8: Test grammar learning on MST-parses\n",
    "This is a continuation of proof-of-concept (POC) experiments in unsupervised language learning (ULL), the OpenCog project hosted on [GitHub](https://github.com/opencog/language-learning/tree/master/notebooks).  \n",
    "This notebook contains tests for MST-parses from OpenCog MST parser 2018-03-16 ([input_data](http://88.99.210.144/data/clustering_2018/input_data/)), results shared via [http://88.99.210.144/data/clustering_2018/POC-Turtle-8-2018-03-16/](http://88.99.210.144/data/clustering_2018/POC-Turtle-8-2018-03-16/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-16 18:26:12 UTC :: module_path: /home/obaskov/language-learning\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path: sys.path.append(module_path)\n",
    "from src.utl.utl import UTC\n",
    "from src.utl.turtle import html_table\n",
    "print(UTC(), ':: module_path:', module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Settings, parameters, data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory /home/obaskov/language-learning/output/Turtle-8-2018-03-16/ exists\n"
     ]
    }
   ],
   "source": [
    "prj_dir = '../output/Turtle-8-2018-03-16/'  # project directory \n",
    "prefix = ''     # all project files will start with this prefix\n",
    "test_data_path = module_path + '/tests/'\n",
    "verbose = 'max' # printed comments: 'none', 'min', 'max'\n",
    "log = {'project': 'POC-Turtle-8: Test MST parses'}\n",
    "\n",
    "if not os.path.exists(prj_dir):\n",
    "    os.makedirs(prj_dir)\n",
    "    print('Project directory created:', module_path + prj_dir[2:])\n",
    "else: print('Project directory', module_path + prj_dir[2:], 'exists')\n",
    "path = module_path + prj_dir[2:]\n",
    "tmpath = module_path + '/tmp/'\n",
    "input_dir = '/home/obaskov/data/clustering_2018/input_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Test \"POC-Turtle\" MST-parses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file: /home/obaskov/data/clustering_2018/input_data/poc-turtle-parses-window-distance-fmi.txt\n",
      "- \"Turtle\" language corpus:\n",
      "\n",
      "## Parses obtained with window-based pair-counting, which accounts\n",
      "## for distance.\n",
      "## Word-pair counts are counted within a window of size K.\n",
      "## The counts added for a word-pair are equal to K/d, where d is\n",
      "## the distance between the two words.\n",
      "\n",
      "## These parses, are the same for K = {2, 6, 10, 30}, but not for K = 1\n",
      "\n",
      "\n",
      "tuna isa fish .\n",
      "0 ###LEFT-WALL### 1 tuna\n",
      "1 tuna 3 fish\n",
      "2 isa 3 fish\n",
      "3 fish 4 .\n",
      "\n",
      "herring isa fish .\n",
      "0 ###LEFT-WALL### 1 herring\n",
      "1 herring 3 fish\n",
      "2 isa 3 fish\n",
      "3 fish 4 .\n",
      "\n",
      "tuna has fin .\n",
      "0 ###LEFT-WALL### 1 tuna\n",
      "1 tuna 2 has\n",
      "2 has 3 fin\n",
      "3 fin 4 .\n",
      "\n",
      "herring has fin .\n",
      "0 ###LEFT-WALL### 1 herring\n",
      "1 herring 2 has\n",
      "2 has 3 fin\n",
      "3 fin 4 .\n",
      "\n",
      "parrot isa bird .\n",
      "0 ###LEFT-WALL### 1 parrot\n",
      "1 parrot 3 bird\n",
      "2 isa 3 bird\n",
      "3 bird 4 .\n",
      "\n",
      "eagle isa bird .\n",
      "0 ###LEFT-WALL### 1 eagle\n",
      "1 eagle 3 bird\n",
      "2 isa 3 bird\n",
      "3 bird 4 .\n",
      "\n",
      "parrot has wing .\n",
      "0 ###LEFT-WALL### 1 parrot\n",
      "1 parrot 2 has\n",
      "2 has 3 wing\n",
      "3 wing 4 .\n",
      "\n",
      "eagle has wing .\n",
      "0 ###LEFT-WALL### 1 eagle\n",
      "1 eagle 2 has\n",
      "2 has 3 wing\n",
      "3 wing 4 .\n",
      "\n",
      "fin isa extremity .\n",
      "0 ###LEFT-WALL### 1 fin\n",
      "1 fin 3 extremity\n",
      "2 isa 3 extremity\n",
      "3 extremity 4 .\n",
      "\n",
      "wing isa extremity .\n",
      "0 ###LEFT-WALL### 1 wing\n",
      "1 wing 3 extremity\n",
      "2 isa 3 extremity\n",
      "3 extremity 4 .\n",
      "\n",
      "fin has scale .\n",
      "0 ###LEFT-WALL### 1 fin\n",
      "1 fin 3 scale\n",
      "2 has 3 scale\n",
      "3 scale 4 .\n",
      "\n",
      "wing has feather .\n",
      "0 ###LEFT-WALL### 1 wing\n",
      "1 wing 3 feather\n",
      "2 has 3 feather\n",
      "3 feather 4 .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_input_file(input_file, verbose='none'):\n",
    "    if os.path.isfile(input_file):\n",
    "        print('Data file:', input_file)\n",
    "        log.update({'input_file': input_file})\n",
    "        if verbose == 'max':\n",
    "            print('- \"Turtle\" language corpus:\\n')\n",
    "            with open(input_file, 'r') as f: \n",
    "                lines = f.read().splitlines()\n",
    "            for line in lines: print(line)\n",
    "        elif verbose not in ['none', 'min']: \n",
    "            print('Input file:', input_file)\n",
    "        return True\n",
    "    else: \n",
    "        print('No data file', input_file)\n",
    "        return False\n",
    "\n",
    "input_file = input_dir+'poc-turtle-parses-window-distance-fmi.txt'\n",
    "check_input_file(input_file, verbose='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 unique disjuncts form 40 unique word-disjunct pairs from 55 parsed items\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>germs</th>\n",
       "      <th>disjuncts</th>\n",
       "      <th>counts</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C01</th>\n",
       "      <td>[.]</td>\n",
       "      <td>[bird-, extremity-, fin-, fish-, scale-, wing-]</td>\n",
       "      <td>11</td>\n",
       "      <td>C01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C02</th>\n",
       "      <td>[LEFT-WALL]</td>\n",
       "      <td>[eagle+, fin+, herring+, parrot+, tuna+, wing+]</td>\n",
       "      <td>11</td>\n",
       "      <td>C02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C03</th>\n",
       "      <td>[bird]</td>\n",
       "      <td>[eagle- &amp; isa- &amp; .+, isa- &amp; parrot- &amp; .+]</td>\n",
       "      <td>2</td>\n",
       "      <td>C03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C04</th>\n",
       "      <td>[eagle, parrot]</td>\n",
       "      <td>[LEFT-WALL- &amp; bird+, LEFT-WALL- &amp; has+]</td>\n",
       "      <td>4</td>\n",
       "      <td>C04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C05</th>\n",
       "      <td>[extremity]</td>\n",
       "      <td>[fin- &amp; isa- &amp; .+, isa- &amp; wing- &amp; .+]</td>\n",
       "      <td>2</td>\n",
       "      <td>C05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C06</th>\n",
       "      <td>[fin]</td>\n",
       "      <td>[LEFT-WALL- &amp; extremity+, LEFT-WALL- &amp; scale+,...</td>\n",
       "      <td>4</td>\n",
       "      <td>C06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C07</th>\n",
       "      <td>[fish]</td>\n",
       "      <td>[herring- &amp; isa- &amp; .+, isa- &amp; tuna- &amp; .+]</td>\n",
       "      <td>2</td>\n",
       "      <td>C07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C08</th>\n",
       "      <td>[has]</td>\n",
       "      <td>[eagle- &amp; wing+, herring- &amp; fin+, parrot- &amp; wi...</td>\n",
       "      <td>5</td>\n",
       "      <td>C08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C09</th>\n",
       "      <td>[herring, tuna]</td>\n",
       "      <td>[LEFT-WALL- &amp; fish+, LEFT-WALL- &amp; has+]</td>\n",
       "      <td>4</td>\n",
       "      <td>C09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C10</th>\n",
       "      <td>[isa]</td>\n",
       "      <td>[bird+, extremity+, fish+]</td>\n",
       "      <td>6</td>\n",
       "      <td>C10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C11</th>\n",
       "      <td>[scale]</td>\n",
       "      <td>[fin- &amp; has- &amp; .+]</td>\n",
       "      <td>1</td>\n",
       "      <td>C11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C12</th>\n",
       "      <td>[wing]</td>\n",
       "      <td>[LEFT-WALL- &amp; extremity+, has- &amp; .+]</td>\n",
       "      <td>3</td>\n",
       "      <td>C12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               germs                                          disjuncts  \\\n",
       "C01              [.]    [bird-, extremity-, fin-, fish-, scale-, wing-]   \n",
       "C02      [LEFT-WALL]    [eagle+, fin+, herring+, parrot+, tuna+, wing+]   \n",
       "C03           [bird]          [eagle- & isa- & .+, isa- & parrot- & .+]   \n",
       "C04  [eagle, parrot]            [LEFT-WALL- & bird+, LEFT-WALL- & has+]   \n",
       "C05      [extremity]              [fin- & isa- & .+, isa- & wing- & .+]   \n",
       "C06            [fin]  [LEFT-WALL- & extremity+, LEFT-WALL- & scale+,...   \n",
       "C07           [fish]          [herring- & isa- & .+, isa- & tuna- & .+]   \n",
       "C08            [has]  [eagle- & wing+, herring- & fin+, parrot- & wi...   \n",
       "C09  [herring, tuna]            [LEFT-WALL- & fish+, LEFT-WALL- & has+]   \n",
       "C10            [isa]                         [bird+, extremity+, fish+]   \n",
       "C11          [scale]                                 [fin- & has- & .+]   \n",
       "C12           [wing]               [LEFT-WALL- & extremity+, has- & .+]   \n",
       "\n",
       "     counts cluster  \n",
       "C01      11     C01  \n",
       "C02      11     C02  \n",
       "C03       2     C03  \n",
       "C04       4     C04  \n",
       "C05       2     C05  \n",
       "C06       4     C06  \n",
       "C07       2     C07  \n",
       "C08       5     C08  \n",
       "C09       4     C09  \n",
       "C10       6     C10  \n",
       "C11       1     C11  \n",
       "C12       3     C12  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mst2stalks(input_file, left_wall='', period=False, verbose='none'):\n",
    "    from src.space.turtle import mst2disjuncts\n",
    "    from src.link_grammar.turtle import lexical_entries, entries2clusters, \\\n",
    "         disjuncts2clusters, entries2rules, save_link_grammar\n",
    "    parses = mst2disjuncts(input_file, lw=left_wall, dot=period)\n",
    "    disjuncts = parses.groupby(['word','disjunct'], as_index=False).sum() \\\n",
    "        .sort_values(by=['count','word','disjunct'], ascending=[False,True,True]) \\\n",
    "        .reset_index(drop=True)\n",
    "    dj_number = len(set(disjuncts['disjunct'].tolist()))\n",
    "    if verbose != 'none': print(dj_number, 'unique disjuncts form', \\\n",
    "        len(disjuncts),'unique word-disjunct pairs from', len(parses), 'parsed items') \n",
    "    return entries2clusters(lexical_entries(disjuncts))\n",
    "\n",
    "stalks = mst2stalks(input_file, left_wall='', period=False, verbose='max')\n",
    "stalks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>Cluster</td><td>Germs</td><td></td><td></td><td>Disjuncts</td></tr><tr><td>C01</td><td>['.']</td><td>[]</td><td>[]</td><td>['C03C01-', 'C05C01-', 'C06C01-', 'C07C01-', 'C11C01-', 'C12C01-']</td></tr><tr><td>C02</td><td>['LEFT-WALL']</td><td>[]</td><td>[]</td><td>['C02C04+', 'C02C06+', 'C02C09+', 'C02C12+']</td></tr><tr><td>C03</td><td>['bird']</td><td>[]</td><td>[]</td><td>['C04C03- & C10C03- & C03C01+', 'C10C03- & C04C03- & C03C01+']</td></tr><tr><td>C04</td><td>['eagle', 'parrot']</td><td>[]</td><td>[]</td><td>['C02C04- & C04C03+', 'C02C04- & C04C08+']</td></tr><tr><td>C05</td><td>['extremity']</td><td>[]</td><td>[]</td><td>['C06C05- & C10C05- & C05C01+', 'C10C05- & C12C05- & C05C01+']</td></tr><tr><td>C06</td><td>['fin']</td><td>[]</td><td>[]</td><td>['C02C06- & C06C05+', 'C02C06- & C06C11+', 'C08C06- & C06C01+']</td></tr><tr><td>C07</td><td>['fish']</td><td>[]</td><td>[]</td><td>['C09C07- & C10C07- & C07C01+', 'C10C07- & C09C07- & C07C01+']</td></tr><tr><td>C08</td><td>['has']</td><td>[]</td><td>[]</td><td>['C04C08- & C08C12+', 'C08C11+', 'C09C08- & C08C06+']</td></tr><tr><td>C09</td><td>['herring', 'tuna']</td><td>[]</td><td>[]</td><td>['C02C09- & C09C07+', 'C02C09- & C09C08+']</td></tr><tr><td>C10</td><td>['isa']</td><td>[]</td><td>[]</td><td>['C10C03+', 'C10C05+', 'C10C07+']</td></tr><tr><td>C11</td><td>['scale']</td><td>[]</td><td>[]</td><td>['C06C11- & C08C11- & C11C01+']</td></tr><tr><td>C12</td><td>['wing']</td><td>[]</td><td>[]</td><td>['C02C12- & C12C05+', 'C08C12- & C12C01+']</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.link_grammar.turtle import disjuncts2clusters, entries2rules\n",
    "rule_list = entries2rules(disjuncts2clusters(stalks))\n",
    "display(html_table([['Cluster','Germs','','','Disjuncts']] + rule_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% POC Turtle Link Grammar v.0.7 2018-03-16 18:26:12 UTC\n",
      "<dictionary-version-number>: V0v0v7+;\n",
      "<dictionary-locale>: EN4us+;\n",
      "\n",
      "% C01\n",
      "\".\":\n",
      "(C03C01-) or (C05C01-) or (C06C01-) or (C07C01-) or (C11C01-) or (C12C01-);\n",
      "\n",
      "% C02\n",
      "\"LEFT-WALL\":\n",
      "(C02C04+) or (C02C06+) or (C02C09+) or (C02C12+);\n",
      "\n",
      "% C03\n",
      "\"bird\":\n",
      "(C04C03- & C10C03- & C03C01+) or (C10C03- & C04C03- & C03C01+);\n",
      "\n",
      "% C04\n",
      "\"eagle\" \"parrot\":\n",
      "(C02C04- & C04C03+) or (C02C04- & C04C08+);\n",
      "\n",
      "% C05\n",
      "\"extremity\":\n",
      "(C06C05- & C10C05- & C05C01+) or (C10C05- & C12C05- & C05C01+);\n",
      "\n",
      "% C06\n",
      "\"fin\":\n",
      "(C02C06- & C06C05+) or (C02C06- & C06C11+) or (C08C06- & C06C01+);\n",
      "\n",
      "% C07\n",
      "\"fish\":\n",
      "(C09C07- & C10C07- & C07C01+) or (C10C07- & C09C07- & C07C01+);\n",
      "\n",
      "% C08\n",
      "\"has\":\n",
      "(C04C08- & C08C12+) or (C08C11+) or (C09C08- & C08C06+);\n",
      "\n",
      "% C09\n",
      "\"herring\" \"tuna\":\n",
      "(C02C09- & C09C07+) or (C02C09- & C09C08+);\n",
      "\n",
      "% C10\n",
      "\"isa\":\n",
      "(C10C03+) or (C10C05+) or (C10C07+);\n",
      "\n",
      "% C11\n",
      "\"scale\":\n",
      "(C06C11- & C08C11- & C11C01+);\n",
      "\n",
      "% C12\n",
      "\"wing\":\n",
      "(C02C12- & C12C05+) or (C08C12- & C12C01+);\n",
      "\n",
      "UNKNOWN-WORD: XXX+;\n",
      "\n",
      "% 12 word clusters, 12 Link Grammar rules.\n",
      "% Link Grammar file saved to: /home/obaskov/language-learning/output/Turtle-8-2018-03-16/poc-turtle_12C_2018-03-16_0007.4.0.dict\n"
     ]
    }
   ],
   "source": [
    "from src.link_grammar.turtle import save_link_grammar\n",
    "lg_file_string = save_link_grammar(rule_list, path)\n",
    "for line in lg_file_string.splitlines(): print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Grammar parsing tests with the dictionary learned from MST-parses.\n",
    "The learned Link Grammar dictionary was used with Link Grammar parser to parse the 12 sentences of the \"Turtle\" corpus: \n",
    "```\n",
    "tuna isa fish. : Found 1 linkage (1 had no P.P. violations)`\n",
    "\n",
    "            +--C09C07--+       \n",
    "    +-C02C09+   +C10C07+C07C01+\n",
    "    |       |   |      |      |\n",
    "LEFT-WALL tuna isa   fish     . \n",
    "\n",
    "\n",
    "herring isa fish. : Found 1 linkage (1 had no P.P. violations)`\n",
    "\n",
    "             +---C09C07---+       \n",
    "    +-C02C09-+     +C10C07+C07C01+\n",
    "    |        |     |      |      |\n",
    "LEFT-WALL herring isa   fish     . \n",
    "\n",
    "\n",
    "tuna has fin. : Found 1 linkage (1 had no P.P. violations)`\n",
    "\n",
    "    +-C02C09+C09C08+C08C06+C06C01+\n",
    "    |       |      |      |      |\n",
    "LEFT-WALL tuna    has    fin     . \n",
    "\n",
    "\n",
    "herring has fin. : Found 1 linkage (1 had no P.P. violations)`\n",
    "\n",
    "    +-C02C09-+C09C08+C08C06+C06C01+\n",
    "    |        |      |      |      |\n",
    "LEFT-WALL herring  has    fin     . \n",
    "\n",
    "\n",
    "parrot isa bird. : Found 1 linkage (1 had no P.P. violations)`\n",
    "\n",
    "             +---C04C03--+       \n",
    "    +-C02C04-+    +C10C03+C03C01+\n",
    "    |        |    |      |      |\n",
    "LEFT-WALL parrot isa   bird     . \n",
    "\n",
    "\n",
    "eagle isa bird. : Found 1 linkage (1 had no P.P. violations)`\n",
    "\n",
    "            +---C04C03--+       \n",
    "    +-C02C04+    +C10C03+C03C01+\n",
    "    |       |    |      |      |\n",
    "LEFT-WALL eagle isa   bird     . \n",
    "\n",
    "\n",
    "parrot has wing. : Found 1 linkage (1 had no P.P. violations)`\n",
    "\n",
    "    +-C02C04-+C04C08+C08C12+C12C01+\n",
    "    |        |      |      |      |\n",
    "LEFT-WALL parrot   has   wing     . \n",
    "\n",
    "\n",
    "eagle has wing. : Found 1 linkage (1 had no P.P. violations)`\n",
    "\n",
    "    +-C02C04+C04C08+C08C12+C12C01+\n",
    "    |       |      |      |      |\n",
    "LEFT-WALL eagle   has   wing     . \n",
    "\n",
    "\n",
    "fin isa extremity. : Found 1 linkage (1 had no P.P. violations)`\n",
    "\n",
    "LEFT-WALL [fin] [isa] [extremity.] \n",
    "\n",
    "\n",
    "wing isa extremity. : Found 1 linkage (1 had no P.P. violations)`\n",
    "\n",
    "            +--C12C05--+       \n",
    "    +-C02C12+   +C10C05+C05C01+\n",
    "    |       |   |      |      |\n",
    "LEFT-WALL wing isa extremity  . \n",
    "\n",
    "\n",
    "fin has scale. : Found 1 linkage (1 had no P.P. violations)`\n",
    "\n",
    "LEFT-WALL [fin] [has] [scale.] \n",
    "\n",
    "\n",
    "wing has feather. : Found 1 linkage (1 had no P.P. violations)`\n",
    "\n",
    "LEFT-WALL [wing] [has] [feather.] \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 First tests on POC-English corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file: /home/obaskov/data/clustering_2018/input_data/poc-english_noCaps-parses-window30-distance-fmi.txt\n",
      "177 unique disjuncts form 229 unique word-disjunct pairs from 515 parsed items\n",
      "\n",
      "Link Grammar rules:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Cluster</td><td>Germs</td><td></td><td></td><td>Disjuncts</td></tr><tr><td>C01</td><td>['.']</td><td>[]</td><td>[]</td><td>['C05C01-', 'C06C01-', 'C07C01-', 'C09C01-', 'C11C01-', 'C15C01-', 'C17C01-', 'C27C01-', 'C29C01-', 'C30C01-', 'C33C01-', 'C35C01-', 'C38C01-', 'C44C01-']</td></tr><tr><td>C02</td><td>['LEFT-WALL']</td><td>[]</td><td>[]</td><td>['C02C03+', 'C02C03+ & C02C13+', 'C02C03+ & C02C25+', 'C02C04+', 'C02C09+', 'C02C13+', 'C02C14+', 'C02C16+', 'C02C25+', 'C02C30+', 'C02C34+']</td></tr><tr><td>C03</td><td>['a']</td><td>[]</td><td>[]</td><td>['C02C03-', 'C02C03- & C03C11+', 'C02C03- & C03C29+', 'C02C03- & C03C38+', 'C03C14+', 'C03C15+', 'C03C34+', 'C03C38+', 'C04C03-', 'C18C03-', 'C21C03-', 'C21C03- & C03C15+', 'C26C03-', 'C40C03-', 'C40C03- & C03C14+', 'C41C03-']</td></tr><tr><td>C04</td><td>['are']</td><td>[]</td><td>[]</td><td>['C02C04- & C06C04- & C04C03+ & C04C38+']</td></tr><tr><td>C05</td><td>['before']</td><td>[]</td><td>[]</td><td>['C15C05- & C40C05- & C05C01+', 'C23C05- & C05C01+', 'C29C05- & C05C01+', 'C40C05- & C05C01+']</td></tr><tr><td>C06</td><td>['binoculars']</td><td>[]</td><td>[]</td><td>['C06C04+', 'C18C06- & C06C01+', 'C41C06- & C06C01+']</td></tr><tr><td>C07</td><td>['board']</td><td>[]</td><td>[]</td><td>['C28C07- & C07C01+', 'C28C07- & C07C36+', 'C37C07- & C07C36+']</td></tr><tr><td>C08</td><td>['by']</td><td>[]</td><td>[]</td><td>['C43C08- & C08C10+']</td></tr><tr><td>C09</td><td>['cake', 'sausage']</td><td>[]</td><td>[]</td><td>['C02C09- & C09C15+', 'C02C09- & C09C40+', 'C23C09-', 'C24C09- & C09C01+', 'C24C09- & C09C27+']</td></tr><tr><td>C10</td><td>['chalk']</td><td>[]</td><td>[]</td><td>['C08C10- & C10C28+']</td></tr><tr><td>C11</td><td>['child', 'human']</td><td>[]</td><td>[]</td><td>['C03C11- & C21C11- & C11C01+', 'C21C11- & C11C27+']</td></tr><tr><td>C12</td><td>['comes']</td><td>[]</td><td>[]</td><td>['C13C12- & C12C28+', 'C14C12- & C12C28+', 'C25C12- & C12C28+', 'C34C12- & C12C28+']</td></tr><tr><td>C13</td><td>['dad']</td><td>[]</td><td>[]</td><td>['C02C13- & C13C12+', 'C02C13- & C13C18+ & C13C30+', 'C02C13- & C13C21+', 'C02C13- & C13C22+', 'C02C13- & C13C23+', 'C02C13- & C13C24+', 'C02C13- & C13C30+ & C13C41+', 'C02C13- & C13C31+', 'C02C13- & C13C32+', 'C02C13- & C13C39+', 'C02C13- & C13C40+', 'C02C13- & C13C41+', 'C02C13- & C13C43+', 'C02C13- & C13C44+', 'C13C21+', 'C13C37+', 'C30C13-', 'C32C13-']</td></tr><tr><td>C14</td><td>['daughter']</td><td>[]</td><td>[]</td><td>['C02C14- & C03C14- & C14C24+', 'C02C14- & C14C12+', 'C02C14- & C14C21+', 'C02C14- & C14C24+', 'C02C14- & C14C43+', 'C03C14-', 'C14C21+', 'C19C14-']</td></tr><tr><td>C15</td><td>['food']</td><td>[]</td><td>[]</td><td>['C03C15- & C09C15- & C15C01+', 'C03C15- & C09C15- & C15C27+', 'C03C15- & C15C05+']</td></tr><tr><td>C16</td><td>['hammed']</td><td>[]</td><td>[]</td><td>['C02C16- & C16C21+ & C16C38+']</td></tr><tr><td>C17</td><td>['hammer']</td><td>[]</td><td>[]</td><td>['C41C17- & C17C01+']</td></tr><tr><td>C18</td><td>['has']</td><td>[]</td><td>[]</td><td>['C13C18- & C18C03+', 'C25C18- & C18C03+ & C18C06+', 'C25C18- & C18C03+ & C18C35+']</td></tr><tr><td>C19</td><td>['her']</td><td>[]</td><td>[]</td><td>['C39C19- & C19C14+ & C19C37+']</td></tr><tr><td>C20</td><td>['his']</td><td>[]</td><td>[]</td><td>['C39C20- & C20C34+ & C20C37+']</td></tr><tr><td>C21</td><td>['is']</td><td>[]</td><td>[]</td><td>['C13C21- & C21C03+ & C21C11+', 'C13C21- & C21C03+ & C21C29+', 'C14C21- & C21C03+ & C21C11+', 'C16C21-', 'C21C03+', 'C21C38+', 'C25C21- & C21C03+ & C21C11+', 'C25C21- & C21C03+ & C21C29+', 'C34C21- & C21C03+ & C21C11+', 'C35C21-']</td></tr><tr><td>C22</td><td>['knocked']</td><td>[]</td><td>[]</td><td>['C13C22- & C22C42+', 'C25C22- & C22C42+']</td></tr><tr><td>C23</td><td>['liked']</td><td>[]</td><td>[]</td><td>['C13C23- & C23C05+ & C23C09+', 'C25C23- & C23C05+ & C23C09+']</td></tr><tr><td>C24</td><td>['likes']</td><td>[]</td><td>[]</td><td>['C13C24- & C24C09+', 'C14C24- & C24C09+', 'C25C24- & C24C09+', 'C34C24- & C24C09+']</td></tr><tr><td>C25</td><td>['mom']</td><td>[]</td><td>[]</td><td>['C02C25- & C25C12+', 'C02C25- & C25C18+', 'C02C25- & C25C21+', 'C02C25- & C25C22+', 'C02C25- & C25C23+', 'C02C25- & C25C24+', 'C02C25- & C25C30+ & C25C31+', 'C02C25- & C25C30+ & C25C41+', 'C02C25- & C25C32+', 'C02C25- & C25C39+', 'C02C25- & C25C40+', 'C02C25- & C25C41+', 'C02C25- & C25C43+', 'C02C25- & C25C44+', 'C25C21+', 'C25C37+', 'C25C44+', 'C30C25-', 'C32C25-']</td></tr><tr><td>C26</td><td>['not']</td><td>[]</td><td>[]</td><td>['C40C26- & C26C03+ & C26C29+']</td></tr><tr><td>C27</td><td>['now']</td><td>[]</td><td>[]</td><td>['C09C27- & C27C01+', 'C11C27- & C27C01+', 'C15C27- & C27C01+', 'C29C27- & C27C01+', 'C32C27- & C27C01+']</td></tr><tr><td>C28</td><td>['on']</td><td>[]</td><td>[]</td><td>['C10C28- & C28C07+ & C28C36+', 'C12C28- & C28C07+']</td></tr><tr><td>C29</td><td>['parent']</td><td>[]</td><td>[]</td><td>['C03C29- & C21C29- & C29C01+', 'C21C29- & C29C27+', 'C26C29- & C29C05+']</td></tr><tr><td>C30</td><td>['saw']</td><td>[]</td><td>[]</td><td>['C02C30- & C30C38+', 'C13C30- & C30C01+', 'C25C30- & C30C01+', 'C30C13+ & C30C41+', 'C30C13+ & C30C44+', 'C30C25+ & C30C41+', 'C30C44+', 'C31C30- & C30C01+']</td></tr><tr><td>C31</td><td>['sawed']</td><td>[]</td><td>[]</td><td>['C13C31- & C31C30+ & C31C42+', 'C25C31- & C31C42+']</td></tr><tr><td>C32</td><td>['sees']</td><td>[]</td><td>[]</td><td>['C13C32- & C32C25+ & C32C27+', 'C25C32- & C32C13+ & C32C27+']</td></tr><tr><td>C33</td><td>['ship']</td><td>[]</td><td>[]</td><td>['C36C33- & C33C01+']</td></tr><tr><td>C34</td><td>['son']</td><td>[]</td><td>[]</td><td>['C02C34- & C03C34- & C34C24+', 'C02C34- & C34C12+', 'C02C34- & C34C21+', 'C02C34- & C34C24+', 'C02C34- & C34C43+', 'C20C34-', 'C34C21+', 'C40C34-']</td></tr><tr><td>C35</td><td>['telescope']</td><td>[]</td><td>[]</td><td>['C18C35- & C35C01+', 'C35C21+ & C35C38+', 'C41C35- & C35C01+']</td></tr><tr><td>C36</td><td>['the']</td><td>[]</td><td>[]</td><td>['C07C36- & C36C33+', 'C28C36-', 'C36C42+']</td></tr><tr><td>C37</td><td>['to']</td><td>[]</td><td>[]</td><td>['C13C37- & C39C37- & C37C07+', 'C19C37- & C37C07+', 'C20C37- & C37C07+', 'C25C37- & C39C37- & C37C07+']</td></tr><tr><td>C38</td><td>['tool']</td><td>[]</td><td>[]</td><td>['C03C38- & C16C38- & C38C01+', 'C03C38- & C21C38- & C30C38- & C38C01+', 'C03C38- & C35C38- & C38C01+', 'C04C38- & C38C01+']</td></tr><tr><td>C39</td><td>['wants']</td><td>[]</td><td>[]</td><td>['C13C39- & C39C20+', 'C13C39- & C39C37+', 'C25C39- & C39C19+', 'C25C39- & C39C37+']</td></tr><tr><td>C40</td><td>['was']</td><td>[]</td><td>[]</td><td>['C09C40- & C40C05+', 'C13C40- & C40C03+ & C40C05+ & C40C34+', 'C13C40- & C40C26+', 'C25C40- & C40C03+ & C40C05+', 'C25C40- & C40C26+']</td></tr><tr><td>C41</td><td>['with']</td><td>[]</td><td>[]</td><td>['C13C41- & C30C41- & C41C03+', 'C13C41- & C30C41- & C41C03+ & C41C06+', 'C13C41- & C30C41- & C41C03+ & C41C17+', 'C13C41- & C30C41- & C41C03+ & C41C35+', 'C25C41- & C30C41- & C41C03+', 'C25C41- & C30C41- & C41C03+ & C41C06+', 'C25C41- & C30C41- & C41C03+ & C41C17+', 'C25C41- & C30C41- & C41C03+ & C41C35+', 'C42C41- & C41C03+', 'C42C41- & C41C03+ & C41C17+']</td></tr><tr><td>C42</td><td>['wood']</td><td>[]</td><td>[]</td><td>['C22C42- & C36C42- & C42C41+', 'C31C42- & C36C42- & C42C41+']</td></tr><tr><td>C43</td><td>['writes']</td><td>[]</td><td>[]</td><td>['C13C43- & C43C08+', 'C14C43- & C43C08+', 'C25C43- & C43C08+', 'C34C43- & C43C08+']</td></tr><tr><td>C44</td><td>['yesterday']</td><td>[]</td><td>[]</td><td>['C13C44- & C25C44- & C30C44- & C44C01+', 'C25C44- & C30C44- & C44C01+']</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Link Grammar dictionary:\n",
      "\n",
      "% POC Turtle Link Grammar v.0.7 2018-03-16 18:26:13 UTC\n",
      "<dictionary-version-number>: V0v0v7+;\n",
      "<dictionary-locale>: EN4us+;\n",
      "\n",
      "% C01\n",
      "\".\":\n",
      "(C05C01-) or (C06C01-) or (C07C01-) or (C09C01-) or (C11C01-) or (C15C01-) or (C17C01-) or (C27C01-) or (C29C01-) or (C30C01-) or (C33C01-) or (C35C01-) or (C38C01-) or (C44C01-);\n",
      "\n",
      "% C02\n",
      "\"LEFT-WALL\":\n",
      "(C02C03+) or (C02C03+ & C02C13+) or (C02C03+ & C02C25+) or (C02C04+) or (C02C09+) or (C02C13+) or (C02C14+) or (C02C16+) or (C02C25+) or (C02C30+) or (C02C34+);\n",
      "\n",
      "% C03\n",
      "\"a\":\n",
      "(C02C03-) or (C02C03- & C03C11+) or (C02C03- & C03C29+) or (C02C03- & C03C38+) or (C03C14+) or (C03C15+) or (C03C34+) or (C03C38+) or (C04C03-) or (C18C03-) or (C21C03-) or (C21C03- & C03C15+) or (C26C03-) or (C40C03-) or (C40C03- & C03C14+) or (C41C03-);\n",
      "\n",
      "% C04\n",
      "\"are\":\n",
      "(C02C04- & C06C04- & C04C03+ & C04C38+);\n",
      "\n",
      "% C05\n",
      "\"before\":\n",
      "(C15C05- & C40C05- & C05C01+) or (C23C05- & C05C01+) or (C29C05- & C05C01+) or (C40C05- & C05C01+);\n",
      "\n",
      "% C06\n",
      "\"binoculars\":\n",
      "(C06C04+) or (C18C06- & C06C01+) or (C41C06- & C06C01+);\n",
      "\n",
      "% C07\n",
      "\"board\":\n",
      "(C28C07- & C07C01+) or (C28C07- & C07C36+) or (C37C07- & C07C36+);\n",
      "\n",
      "% C08\n",
      "\"by\":\n",
      "(C43C08- & C08C10+);\n",
      "\n",
      "% C09\n",
      "\"cake\" \"sausage\":\n",
      "(C02C09- & C09C15+) or (C02C09- & C09C40+) or (C23C09-) or (C24C09- & C09C01+) or (C24C09- & C09C27+);\n",
      "\n",
      "% C10\n",
      "\"chalk\":\n",
      "(C08C10- & C10C28+);\n",
      "\n",
      "% C11\n",
      "\"child\" \"human\":\n",
      "(C03C11- & C21C11- & C11C01+) or (C21C11- & C11C27+);\n",
      "\n",
      "% C12\n",
      "\"comes\":\n",
      "(C13C12- & C12C28+) or (C14C12- & C12C28+) or (C25C12- & C12C28+) or (C34C12- & C12C28+);\n",
      "\n",
      "% C13\n",
      "\"dad\":\n",
      "(C02C13- & C13C12+) or (C02C13- & C13C18+ & C13C30+) or (C02C13- & C13C21+) or (C02C13- & C13C22+) or (C02C13- & C13C23+) or (C02C13- & C13C24+) or (C02C13- & C13C30+ & C13C41+) or (C02C13- & C13C31+) or (C02C13- & C13C32+) or (C02C13- & C13C39+) or (C02C13- & C13C40+) or (C02C13- & C13C41+) or (C02C13- & C13C43+) or (C02C13- & C13C44+) or (C13C21+) or (C13C37+) or (C30C13-) or (C32C13-);\n",
      "\n",
      "% C14\n",
      "\"daughter\":\n",
      "(C02C14- & C03C14- & C14C24+) or (C02C14- & C14C12+) or (C02C14- & C14C21+) or (C02C14- & C14C24+) or (C02C14- & C14C43+) or (C03C14-) or (C14C21+) or (C19C14-);\n",
      "\n",
      "% C15\n",
      "\"food\":\n",
      "(C03C15- & C09C15- & C15C01+) or (C03C15- & C09C15- & C15C27+) or (C03C15- & C15C05+);\n",
      "\n",
      "% C16\n",
      "\"hammed\":\n",
      "(C02C16- & C16C21+ & C16C38+);\n",
      "\n",
      "% C17\n",
      "\"hammer\":\n",
      "(C41C17- & C17C01+);\n",
      "\n",
      "% C18\n",
      "\"has\":\n",
      "(C13C18- & C18C03+) or (C25C18- & C18C03+ & C18C06+) or (C25C18- & C18C03+ & C18C35+);\n",
      "\n",
      "% C19\n",
      "\"her\":\n",
      "(C39C19- & C19C14+ & C19C37+);\n",
      "\n",
      "% C20\n",
      "\"his\":\n",
      "(C39C20- & C20C34+ & C20C37+);\n",
      "\n",
      "% C21\n",
      "\"is\":\n",
      "(C13C21- & C21C03+ & C21C11+) or (C13C21- & C21C03+ & C21C29+) or (C14C21- & C21C03+ & C21C11+) or (C16C21-) or (C21C03+) or (C21C38+) or (C25C21- & C21C03+ & C21C11+) or (C25C21- & C21C03+ & C21C29+) or (C34C21- & C21C03+ & C21C11+) or (C35C21-);\n",
      "\n",
      "% C22\n",
      "\"knocked\":\n",
      "(C13C22- & C22C42+) or (C25C22- & C22C42+);\n",
      "\n",
      "% C23\n",
      "\"liked\":\n",
      "(C13C23- & C23C05+ & C23C09+) or (C25C23- & C23C05+ & C23C09+);\n",
      "\n",
      "% C24\n",
      "\"likes\":\n",
      "(C13C24- & C24C09+) or (C14C24- & C24C09+) or (C25C24- & C24C09+) or (C34C24- & C24C09+);\n",
      "\n",
      "% C25\n",
      "\"mom\":\n",
      "(C02C25- & C25C12+) or (C02C25- & C25C18+) or (C02C25- & C25C21+) or (C02C25- & C25C22+) or (C02C25- & C25C23+) or (C02C25- & C25C24+) or (C02C25- & C25C30+ & C25C31+) or (C02C25- & C25C30+ & C25C41+) or (C02C25- & C25C32+) or (C02C25- & C25C39+) or (C02C25- & C25C40+) or (C02C25- & C25C41+) or (C02C25- & C25C43+) or (C02C25- & C25C44+) or (C25C21+) or (C25C37+) or (C25C44+) or (C30C25-) or (C32C25-);\n",
      "\n",
      "% C26\n",
      "\"not\":\n",
      "(C40C26- & C26C03+ & C26C29+);\n",
      "\n",
      "% C27\n",
      "\"now\":\n",
      "(C09C27- & C27C01+) or (C11C27- & C27C01+) or (C15C27- & C27C01+) or (C29C27- & C27C01+) or (C32C27- & C27C01+);\n",
      "\n",
      "% C28\n",
      "\"on\":\n",
      "(C10C28- & C28C07+ & C28C36+) or (C12C28- & C28C07+);\n",
      "\n",
      "% C29\n",
      "\"parent\":\n",
      "(C03C29- & C21C29- & C29C01+) or (C21C29- & C29C27+) or (C26C29- & C29C05+);\n",
      "\n",
      "% C30\n",
      "\"saw\":\n",
      "(C02C30- & C30C38+) or (C13C30- & C30C01+) or (C25C30- & C30C01+) or (C30C13+ & C30C41+) or (C30C13+ & C30C44+) or (C30C25+ & C30C41+) or (C30C44+) or (C31C30- & C30C01+);\n",
      "\n",
      "% C31\n",
      "\"sawed\":\n",
      "(C13C31- & C31C30+ & C31C42+) or (C25C31- & C31C42+);\n",
      "\n",
      "% C32\n",
      "\"sees\":\n",
      "(C13C32- & C32C25+ & C32C27+) or (C25C32- & C32C13+ & C32C27+);\n",
      "\n",
      "% C33\n",
      "\"ship\":\n",
      "(C36C33- & C33C01+);\n",
      "\n",
      "% C34\n",
      "\"son\":\n",
      "(C02C34- & C03C34- & C34C24+) or (C02C34- & C34C12+) or (C02C34- & C34C21+) or (C02C34- & C34C24+) or (C02C34- & C34C43+) or (C20C34-) or (C34C21+) or (C40C34-);\n",
      "\n",
      "% C35\n",
      "\"telescope\":\n",
      "(C18C35- & C35C01+) or (C35C21+ & C35C38+) or (C41C35- & C35C01+);\n",
      "\n",
      "% C36\n",
      "\"the\":\n",
      "(C07C36- & C36C33+) or (C28C36-) or (C36C42+);\n",
      "\n",
      "% C37\n",
      "\"to\":\n",
      "(C13C37- & C39C37- & C37C07+) or (C19C37- & C37C07+) or (C20C37- & C37C07+) or (C25C37- & C39C37- & C37C07+);\n",
      "\n",
      "% C38\n",
      "\"tool\":\n",
      "(C03C38- & C16C38- & C38C01+) or (C03C38- & C21C38- & C30C38- & C38C01+) or (C03C38- & C35C38- & C38C01+) or (C04C38- & C38C01+);\n",
      "\n",
      "% C39\n",
      "\"wants\":\n",
      "(C13C39- & C39C20+) or (C13C39- & C39C37+) or (C25C39- & C39C19+) or (C25C39- & C39C37+);\n",
      "\n",
      "% C40\n",
      "\"was\":\n",
      "(C09C40- & C40C05+) or (C13C40- & C40C03+ & C40C05+ & C40C34+) or (C13C40- & C40C26+) or (C25C40- & C40C03+ & C40C05+) or (C25C40- & C40C26+);\n",
      "\n",
      "% C41\n",
      "\"with\":\n",
      "(C13C41- & C30C41- & C41C03+) or (C13C41- & C30C41- & C41C03+ & C41C06+) or (C13C41- & C30C41- & C41C03+ & C41C17+) or (C13C41- & C30C41- & C41C03+ & C41C35+) or (C25C41- & C30C41- & C41C03+) or (C25C41- & C30C41- & C41C03+ & C41C06+) or (C25C41- & C30C41- & C41C03+ & C41C17+) or (C25C41- & C30C41- & C41C03+ & C41C35+) or (C42C41- & C41C03+) or (C42C41- & C41C03+ & C41C17+);\n",
      "\n",
      "% C42\n",
      "\"wood\":\n",
      "(C22C42- & C36C42- & C42C41+) or (C31C42- & C36C42- & C42C41+);\n",
      "\n",
      "% C43\n",
      "\"writes\":\n",
      "(C13C43- & C43C08+) or (C14C43- & C43C08+) or (C25C43- & C43C08+) or (C34C43- & C43C08+);\n",
      "\n",
      "% C44\n",
      "\"yesterday\":\n",
      "(C13C44- & C25C44- & C30C44- & C44C01+) or (C25C44- & C30C44- & C44C01+);\n",
      "\n",
      "UNKNOWN-WORD: XXX+;\n",
      "\n",
      "% 44 word clusters, 44 Link Grammar rules.\n",
      "% Link Grammar file saved to: /home/obaskov/language-learning/output/Turtle-8-2018-03-16/poc-turtle_44C_2018-03-16_0007.4.0.dict\n"
     ]
    }
   ],
   "source": [
    "def pipeline(input_file, left_wall='', period=False, verbose='none'):\n",
    "    from src.space.turtle import mst2disjuncts\n",
    "    from src.link_grammar.turtle import lexical_entries, entries2clusters, \\\n",
    "         disjuncts2clusters, entries2rules, save_link_grammar\n",
    "    # import check_input_file, mst2stalks # this notebook\n",
    "    check_input_file(input_file, verbose=verbose)\n",
    "    stalks = mst2stalks(input_file, left_wall, period, verbose)\n",
    "    if verbose == 'max': print('Stalks:\\n',stalks[['germs','disjuncts']])\n",
    "    rule_list = entries2rules(disjuncts2clusters(stalks))\n",
    "    if verbose != 'none':\n",
    "        print('\\nLink Grammar rules:')\n",
    "        display(html_table([['Cluster','Germs','','','Disjuncts']] + rule_list))\n",
    "    lg_file_string = save_link_grammar(rule_list, path)\n",
    "    if verbose != 'none': # not in ['none', 'min']:\n",
    "        print('\\nLink Grammar dictionary:\\n')\n",
    "        for line in lg_file_string.splitlines(): print(line)\n",
    "    else: \n",
    "        print('\\n'.join(x[2:] for x in lg_file_string.splitlines()[-2:]))\n",
    "\n",
    "input_file = input_dir+'poc-english_noCaps-parses-window30-distance-fmi.txt'\n",
    "pipeline(input_file, left_wall='', period=False, verbose='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR\n",
    "**Good news**: The language learning pipeline manages to learn Link Grammar rules for a more complicated \"POC-English\" corpus.\n",
    "\n",
    "**Bad news**:\n",
    "- The rules learned by collecting multi-germ-multi-disjunct lexical entries are too detailed -- 44 clusters, most consisting of a single word suggest further clustering to provide generalised word categories and grammar rules.\n",
    "- The learned rules could not be verified by the Link Grammar parser (to be amended)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
